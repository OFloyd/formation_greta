{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "077810bf",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149d5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "# import re\n",
    "import random\n",
    "import pickle\n",
    "import datasets as dst\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, auc, roc_curve, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096600b6",
   "metadata": {},
   "source": [
    "# Chargement des données et prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fdfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessDataframe(df: object) -> object:\n",
    "    dfret = df.dropna(subset = [\n",
    "        \"station_id\", \n",
    "        \"lat\", \n",
    "        \"lon\", \n",
    "        \"occupation_prct\", \n",
    "        \"occupation_class\"\n",
    "    ]).copy()\n",
    "    \n",
    "    dfret['jour'] = dfret[\"time\"].dt.day\n",
    "    dfret['heure'] = dfret[\"time\"].dt.hour\n",
    "    dfret['minute'] = dfret[\"time\"].dt.minute\n",
    "    \n",
    "    return dfret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "681ffbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='F:/work/datasets/'\n",
    "df0 = dst.getDataset(\"2023_03_17\", path)\n",
    "df1 = dst.getDataset(\"2023_03_18\", path)\n",
    "df2 = dst.getDataset(\"2023_03_19\", path)\n",
    "df3 = dst.getDataset(\"2023_03_20\", path)\n",
    "df4 = dst.getDataset(\"2023_03_21\", path)\n",
    "df5 = dst.getDataset(\"2023_03_22\", path)\n",
    "\n",
    "#Je réduis systématiquement la résolution en temps par 10 sur ce classeur\n",
    "step=10\n",
    "df0=df0.iloc[0::step]\n",
    "df1=df1.iloc[0::step]\n",
    "df2=df2.iloc[0::step]\n",
    "df3=df3.iloc[0::step]\n",
    "df4=df4.iloc[0::step]\n",
    "df5=df5.iloc[0::step]\n",
    "\n",
    "\n",
    "df0=preProcessDataframe(df0)\n",
    "df1=preProcessDataframe(df1)\n",
    "df2=preProcessDataframe(df2)\n",
    "df3=preProcessDataframe(df3)\n",
    "df4=preProcessDataframe(df4)\n",
    "df5=preProcessDataframe(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4cbf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n"
     ]
    }
   ],
   "source": [
    "#On vérifie que prendre une ligne sur 10 laisse un meme nombre de points par station\n",
    "df=df1\n",
    "station_list=list(set(df.station_id))\n",
    "Nst=len(station_list)\n",
    "Nt=len(set(df[df['station_id'] == station_list[0]].time))\n",
    "station_timeserie=np.ndarray([Nst,Nt])\n",
    "\n",
    "foo=[]\n",
    "for i in station_list : \n",
    "    foo.append(len(set(df[df['station_id'] == i]) ))\n",
    "print(np.min(foo) , np.max(foo) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fbd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e34770",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ajout de valeurs d'occupation 6h plus tot :\n",
    "def add_old_values(df, df_old=None):\n",
    "    dec1=6*3 #on a une valeur toutes les 20 minutes, et on regarde 6 heures avant\n",
    "    dec2=6*3+24*3 #on a une valeur toutes les 20 minutes, et on regarde 6 heures avant\n",
    "    \n",
    "    if df_old is None:\n",
    "        df_main=df.copy()\n",
    "    else : \n",
    "        df_main=pd.concat([df_old, df])\n",
    "        \n",
    "    df_main[\"occupation_diff\"]=df_main[\"occupation_prct\"].shift(periods=dec1, axis=0)-df_main[\"occupation_prct\"].shift(periods=dec2, axis=0)\n",
    "\n",
    "   \n",
    "    df_main=df_main[df_main['jour'] == df['jour'].median()]\n",
    "    \n",
    "    \n",
    "    return df_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf103aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo=df4[df4['station_id'] == station_list[0]]\n",
    "# foo.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7dfd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo=add_old_values(df3, df2)\n",
    "# plt.plot(foo[foo['station_id'] == station_list[0]].occupation_class, color='r')\n",
    "# plt.plot(foo[foo['station_id'] == station_list[0]].old_occupation_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c1c49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=add_old_values(df5, df4)\n",
    "df4=add_old_values(df4, df3)\n",
    "df3=add_old_values(df3, df2)\n",
    "df2=add_old_values(df2, df1)\n",
    "df1=add_old_values(df1, df0)\n",
    "df0=add_old_values(df0)\n",
    "# df0=df0.dropna(subset=['old_occupation_prct','old_occupation_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986d1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d53fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53d90ca2",
   "metadata": {},
   "source": [
    "## Entrainement et prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b36fc5",
   "metadata": {},
   "source": [
    "### Toutes les stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf623b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.concat([df0,df1])\n",
    "df_train=pd.concat([df_train,df2])\n",
    "df_train=pd.concat([df_train,df3])\n",
    "df_train=pd.concat([df_train,df4])\n",
    "# df_tot=pd.concat(df_tot,df4)\n",
    "\n",
    "df_test=df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1 = [\"station_id\", \"lon\", \"lat\", \"jour\", \"heure\", \"minute\"]\n",
    "feat2 = feat1+[\"old_occupation_class\"]\n",
    "target = [\"occupation_class\"]\n",
    "\n",
    "x_train1=df_train[feat1]\n",
    "x_train2=df_train[feat2]\n",
    "y_train=df_train[target]\n",
    "\n",
    "x_test1=df_test[feat1]\n",
    "x_test2=df_test[feat2]\n",
    "y_test=df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e22bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf1.fit(x_train1, y_train.values.ravel())\n",
    "\n",
    "clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf2.fit(x_train2, y_train.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = clf1.predict(x_test1)\n",
    "y_pred2 = clf2.predict(x_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa636fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686bcbb",
   "metadata": {},
   "source": [
    "### Une station à la fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049094f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_class(df_train_in: object, df_test_in, feat_supp=[]) -> None:\n",
    "    \n",
    "    station=random.sample(station_list,1)[0]\n",
    "    print(station)\n",
    "    \n",
    "    \n",
    "    df_train=df_train_in[df_train_in['station_id'] == station]\n",
    "    df_test=df_test_in[df_test_in['station_id'] == station]\n",
    "\n",
    "    feat1 = [\"station_id\", \"lon\", \"lat\", \"jour\", \"heure\", \"minute\"]\n",
    "    feat2 = feat1+[\"old_occupation_prct\", \"old_occupation_class\"]\n",
    "    target = [\"occupation_class\"]\n",
    "\n",
    "    x_train1=df_train[feat1]\n",
    "    x_train2=df_train[feat2]\n",
    "    y_train=df_train[target]\n",
    "\n",
    "    x_test1=df_test[feat1]\n",
    "    x_test2=df_test[feat2]\n",
    "    y_test=df_test[target]\n",
    "\n",
    "    clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    clf1.fit(x_train1, y_train.values.ravel())\n",
    "\n",
    "    clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    clf2.fit(x_train2, y_train.values.ravel())\n",
    "\n",
    "\n",
    "    y_pred1 = clf1.predict(x_test1)\n",
    "    y_pred2 = clf2.predict(x_test2)\n",
    "\n",
    "\n",
    "#     print(feat)\n",
    "#     print(clf.feature_importances_)\n",
    "\n",
    "#     cm1 = confusion_matrix(y_test, y_pred1)\n",
    "    \n",
    "    \n",
    "#     cm2 = confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "#     sns.heatmap(cm1, annot=True, cmap=\"Blues\", fmt=\"g\")\n",
    "#     sns.heatmap(cm2, annot=True, cmap=\"Blues\", fmt=\"g\")    \n",
    "    \n",
    "#     plt.xlabel('Prédictions')\n",
    "#     plt.ylabel('Valeurs réelles')\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(classification_report(y_test,y_pred1))\n",
    "#     print(classification_report(y_test,y_pred2))\n",
    "\n",
    "\n",
    "#     print(len(y_test),len(y_pred1),len(y_pred2))\n",
    "#     print(type(y_test),len(y_pred1),len(y_pred2))\n",
    "\n",
    "    plt.plot(y_test.values.ravel(), color='black', label='True')\n",
    "    plt.plot(y_pred1, color='red', label='Pred. basic')\n",
    "    plt.plot(y_pred2, color='blue', label='Pred. basic+old values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#     print(y_test.values.ravel())\n",
    "#     print('\\n')\n",
    "#     print(y_pred1)\n",
    "#     print('\\n')\n",
    "#     print(y_pred2)\n",
    "\n",
    "# test_one(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_class(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c986f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_regr(df_train_in: object, df_test_in, feat_supp=[]) -> None:\n",
    "    \n",
    "    station=random.sample(station_list,1)[0]\n",
    "    print(station)\n",
    "    \n",
    "    \n",
    "    df_train=df_train_in[df_train_in['station_id'] == station]\n",
    "    df_test=df_test_in[df_test_in['station_id'] == station]\n",
    "\n",
    "    feat1 = [\"station_id\", \"lon\", \"lat\", \"jour\", \"heure\", \"minute\"]\n",
    "    feat2 = feat1+[\"old_occupation_prct\", \"old_occupation_class\"]\n",
    "    target = [\"occupation_prct\"]\n",
    "\n",
    "    x_train1=df_train[feat1]\n",
    "    x_train2=df_train[feat2]\n",
    "    y_train=df_train[target]\n",
    "\n",
    "    x_test1=df_test[feat1]\n",
    "    x_test2=df_test[feat2]\n",
    "    y_test=df_test[target]\n",
    "\n",
    "    clf1 = GradientBoostingRegressor(random_state=0)\n",
    "    clf1.fit(x_train1, y_train.values.ravel())\n",
    "\n",
    "    clf2 = GradientBoostingRegressor(random_state=0)\n",
    "    clf2.fit(x_train2, y_train.values.ravel())\n",
    "\n",
    "\n",
    "    y_pred1 = clf1.predict(x_test1)\n",
    "    y_pred2 = clf2.predict(x_test2)\n",
    "\n",
    "    x=df_test['time']\n",
    "    \n",
    "    print(x.shape, y_test.shape, y_pred1.shape)\n",
    "    \n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.plot(x, y_test.values.ravel(), color='black', label='True')\n",
    "    plt.plot(x, y_pred1, color='red', label='Pred. basic')\n",
    "    plt.plot(x, y_pred2, color='blue', label='Pred. basic+old values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #create a dataframe with the variable importances of the model\n",
    "    df_importances = pd.DataFrame({\n",
    "        'feature': clf1.feature_names_in_,\n",
    "        'importance': clf1.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    #plot variable importances of the model\n",
    "    plt.title('Variable Importances : basic prediction', fontsize=16)\n",
    "    sns.barplot(x = df_importances.importance, y = df_importances.feature, orient='h')\n",
    "    plt.show()\n",
    "    \n",
    "    #create a dataframe with the variable importances of the model\n",
    "    df_importances = pd.DataFrame({\n",
    "        'feature': clf2.feature_names_in_,\n",
    "        'importance': clf2.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    #plot variable importances of the model\n",
    "    plt.title('Variable Importances ; basic + old values', fontsize=16)\n",
    "    sns.barplot(x = df_importances.importance, y = df_importances.feature, orient='h')\n",
    "    plt.show()\n",
    "# test_one(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f1abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_regr(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba98ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd4174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c4cb35a",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b645348",
   "metadata": {},
   "source": [
    "On va essayer d'appliquer une régression par RNN ou un CNN (d'abord CNN car il y a un tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22554595",
   "metadata": {},
   "source": [
    "## Feature et target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bada3d0",
   "metadata": {},
   "source": [
    "On met en entrée les valeurs sur x heures avant le temps de la prédiction. Puis la prédiction est encore quelques heures après (6h par exemple).\n",
    "On fait ça pour chaque temps de la journée.\n",
    "\n",
    "Les features seront les taux d'occupation, le jour de la semaine, l'heure, l'identifiant de la station. On pourra rajouter les taux d'occupation décalés de 6 heures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On suppose qu'on a une donnée toutes les 20 minutes\n",
    "Ntrain=24*3 #On s'entraine sur chaque temps de la journée\n",
    "\n",
    "Nt=6*3  #On regarde 6 heures avant\n",
    "Nfeat=4\n",
    "\n",
    "#Chaque input aura les formes :\n",
    "# Input shape: (32, 18, 4)\n",
    "# Labels shape: (32, 18, 1)\n",
    "# Output shape: (32, 16, 1)\n",
    "\n",
    "Ntrain=df_test['station_id'].count()\n",
    "Ntest=df_test['station_id'].count()\n",
    "\n",
    "\n",
    "\n",
    "# feat2 = feat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ff579",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list=list(set(df_train.station_id))\n",
    "station=random.sample(station_list,1)[0]\n",
    "\n",
    "\n",
    "df_train_one=df_train[df_train['station_id'] == station]\n",
    "df_test_one=df_test[df_test['station_id'] == station]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(station, df_train_one.station_id.count(), df_test_one.station_id.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde76f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = [\"station_id\", \"lon\", \"lat\", \"jour\", \"heure\", \"minute\"]+[\"old_occupation_prct\"]\n",
    "target=[\"occupation_prct\"]\n",
    "\n",
    "def format_input(df, feat, target):\n",
    "    \n",
    "    numarr_feat_in=df[feat].to_numpy()\n",
    "    numarr_targ_in=df[target].to_numpy()\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(numarr_in.shape)\n",
    "\n",
    "    Nt=numarr_feat_in.shape[0]\n",
    "    Nf=numarr_feat_in.shape[1]\n",
    "    Ndt=24*3\n",
    "\n",
    "    numarr_feat_out=np.ndarray([Nt-Ndt,Nf,Ndt])\n",
    "    numarr_targ_out=numarr_targ_in[Ndt:]\n",
    "    \n",
    "    for i in range(Nt-Ndt):\n",
    "        for f in range(Nf):\n",
    "            numarr_feat_out[i,f,:]=numarr_feat_in[i:i+Ndt,f]\n",
    "    \n",
    "    \n",
    "    return numarr_feat_out, numarr_targ_out\n",
    "# #     df_return\n",
    "#     for \n",
    "\n",
    "    \n",
    "#     return df_ret\n",
    "\n",
    "# x_train, y_train=format_input(df_train, feat, target)\n",
    "# x_test, y_test=format_input(df_test, feat, target)\n",
    "x_train, y_train=format_input(df_train_one, feat, target)\n",
    "x_test, y_test=format_input(df_test_one, feat, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list=list(set(df_train.station_id))\n",
    "station=random.sample(station_list,1)[0]\n",
    "\n",
    "df=pd.concat([df_train,df_test])\n",
    "df_one=df[df['station_id'] == station]\n",
    "\n",
    "# print(station, df_train_one.station_id.count(), df_test_one.station_id.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02249691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(df, feat, target):\n",
    "\n",
    "    feat = [\"station_id\", \"lon\", \"lat\", \"jour\", \"heure\", \"minute\",\"old_occupation_prct\"]\n",
    "#     feat = [\"jour\", \"heure\", \"minute\"]+[\"old_occupation_prct\"]\n",
    "    target=[\"occupation_prct\"]\n",
    "    \n",
    "    numarr_feat_in=df[feat].to_numpy()\n",
    "    numarr_targ_in=df[target].to_numpy()\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(numarr_in.shape)\n",
    "\n",
    "    Nt=numarr_feat_in.shape[0]\n",
    "    Nf=numarr_feat_in.shape[1]\n",
    "    Ndt=24*3\n",
    "\n",
    "    numarr_feat_out=np.ndarray([Nt-Ndt,Nf,Ndt])\n",
    "    numarr_targ_out=numarr_targ_in[Ndt:]\n",
    "    \n",
    "    for i in range(Nt-Ndt):\n",
    "        for f in range(Nf):\n",
    "            numarr_feat_out[i,f,:]=numarr_feat_in[i:i+Ndt,f]\n",
    "    \n",
    "#split train test : \n",
    "#     ind=numarr_feat_out.day == numarr_feat_out.day.max()\n",
    "    ind_train=numarr_feat_out[:,3,-1] != 22\n",
    "    ind_test=numarr_feat_out[:,3,-1] == 22\n",
    "#     ind_train=numarr_feat_out[:,0,-1] != 22\n",
    "#     ind_test=numarr_feat_out[:,0,-1] == 22\n",
    "    x_train=numarr_feat_out[ind_train,:,:]\n",
    "    x_test= numarr_feat_out[ind_test,:,:]\n",
    "    y_train=numarr_targ_out[ind_train]\n",
    "    y_test= numarr_targ_out[ind_test]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "#   return numarr_feat_out, numarr_targ_out\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test=format_input(df_one, feat, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870986fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ea5ea67",
   "metadata": {},
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONV_WIDTH=3\n",
    "# conv_model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv1D(filters=32,\n",
    "#                            kernel_size=(CONV_WIDTH,),\n",
    "#                            activation='relu'),\n",
    "#     tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "#     tf.keras.layers.Dense(units=1),\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Activation, Dropout, Input, Flatten\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(7,72,1)) \n",
    "# inputs = Input(shape=(4,72,1)) \n",
    "tens = Conv1D(32, kernel_size = 3, activation='relu' ,)(inputs) \n",
    "# tens = Convolution2D(64, kernel_size = (3, 3), activation='relu' )(tens) \n",
    "# tens = MaxPooling2D((3, 3))(tens) \n",
    "# tens = Flatten()(tens) \n",
    "tens = Dense(32, activation='relu')(tens) \n",
    "tens = Dropout(0.5)(tens) \n",
    "\n",
    "tens = Flatten()(tens) \n",
    "# outputs= Dense(8, activation='softmax')(tens) \n",
    "outputs= Dense(1)(tens) \n",
    "model = Model(inputs, outputs) \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_norm=x_train-np.mean(x_train, axis=0)\n",
    "\n",
    "# x_train_norm=x_train\n",
    "# for i in range(x_train.shape[0]):\n",
    "# #     if np.std(x_train, axis=0) :\n",
    "#     x_train_norm[i,:,:]=(x_train_norm[i,:,:]-np.mean(x_train, axis=0))/np.std(x_train, axis=0)\n",
    "\n",
    "# # x_train_norm\n",
    "\n",
    "# np.mean(x_train)\n",
    "\n",
    "def norm_input(x):\n",
    "    x_norm=x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        div=np.std(x, axis=0)\n",
    "        x_norm[i,:,:]=(x[i,:,:]-np.mean(x, axis=0))\n",
    "        \n",
    "        for j in range(x.shape[1]) :\n",
    "            for k in range(x.shape[2]) :\n",
    "        \n",
    "                if div[j,k] == 0 :\n",
    "                    x_norm[i,j,k]=0\n",
    "                else :\n",
    "                    x_norm[i,j,k]=x_norm[i,j,k]/div[j,k]\n",
    "#         x_norm[i,:,:]=(x[i,:,:]-np.mean(x, axis=0))/div\n",
    "                \n",
    "    return x_norm\n",
    "\n",
    "\n",
    "x_train_norm=norm_input(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "epochs=20\n",
    "early=tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-1,\n",
    "                                                             decay_steps=10000,decay_rate=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr_schedule), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58da30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x_train_norm=x_train-mean\n",
    "tps1 = time.perf_counter()\n",
    "history =model.fit(x_train_norm, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                   verbose=1,validation_data=(x_test, y_test), callbacks=[early]) #, class_weight=class_weight\n",
    "tps2 = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "# print('lr=',lr,'batch_size=',batch_size, 'epochs=',epochs)\n",
    "print('Temps d apprentissage',tps2 - tps1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(norm_input(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e89d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test)\n",
    "# plt.plot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(y_test)\n",
    "plt.plot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[0,3,:])\n",
    "plt.plot(y_test[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d995038",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe981e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[0,3,:])\n",
    "plt.plot(y_train[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c5d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85f339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70838ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482f074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb06da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebc42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=7)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min')\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10,\n",
    "                      validation_data=[x_test, y_test],\n",
    "                      callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd9308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "788px",
    "left": "45px",
    "top": "318px",
    "width": "369.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
